### Kafka Installation

[Apache Kafka](https://kafka.apache.org/) is an open-source stream-processing software platform.

**Create Confluent account**

To begin using the Cloud platform open the [site of confluent](https://www.confluent.io/) to start registration.

{% capture kafka_memo %}
**Memo:** The Confluent interface may change in the future.
{% endcapture %}

{% include templates/info-banner.md content=kafka_memo %}

Click "GET STARTED FREE", then in the loaded page should be selected the **Cloud** tab, then fill in the required fields of the **Sign up** section. 

Confirm the completed fields ("START FREE"). Then use the verification link that was sent to your specified mail.

{% include images-gallery.html imageCollection="Kafka_confluent_registration" %}

<br/>

Create cloud environment and cluster. Add payment method if you don't add earlier yet. 

{% include images-gallery.html imageCollection="Kafka_cloud_environment" %}

<br/>
Before proceeding with adding integrations, you will need to create a suitable **Uplink Converter**.


**Create Uplink Converter**

To create **Uplink converter**, go to the **Data Converters** section and click **Add Data Converter**, then **Create New Converter**. Name it "**Uplink (Kafka)**" and select the **uplink** type. Use debug mode when you need to parse decoder events.

{% capture kafka_please_note %}
**Note:** While debug mode is very useful for development and troubleshooting, leaving it enabled in production mode can significantly increase the disk space used by the database since all debug data is stored there. After debugging is complete, it is highly recommended turning off debug mode.
{% endcapture %}
{% include templates/info-banner.md content=kafka_please_note %}


You can use the following code, copy it to the decoder function section:

```js
// Decode an uplink message from a buffer
// payload - array of bytes
// metadata - key/value object

/** Decoder **/
// decode payload to JSON
var payloadJsn = decodeToJson(payload);

// decode payload to String
// var payloadStr = decodeToString(payload);

// var groupName = 'thermostat devices';
// use assetName and assetType instead of deviceName and deviceType
// to automatically create assets instead of devices.
// var assetName = 'Asset A';
// var assetType = 'building';

// Result object with device/asset attributes/telemetry data
   var result = {
// Use deviceName and deviceType or assetName and assetType, but not both.
   deviceName: payloadJsn.deviceName,
   deviceType: payloadJsn.deviceType,
// assetName: assetName,
// assetType: assetType,
   attributes: payloadJsn.attributes,
   telemetry: payloadJsn.telemetry
};

/** Helper functions **/
function decodeToString(payload) {
   return String.fromCharCode.apply(String, payload);
}

function decodeToJson(payload) {
   // covert payload to string.
   var str = decodeToString(payload);
   // parse string to JSON
   var data = JSON.parse(str);
   return data;
}

return result;
```
{: .copy-code}

Example of payload:
```json
{
        "deviceName":"SN-111",
        "deviceType":"default",
        "attributes":{
            "model":"Model A"
        },
        "telemetry":[
            {
                "ts":1634601341000,
                "values":{
                    "battery":3.99,
                    "temperature":27.05
                }
            },
            {
                "ts":1634608351000,
                "values":{
                    "battery":3.14,
                    "temperature":27.51
        }}]
}
```
{: .copy-code}

{% include images-gallery.html imageCollection="Create Uplink Converter" %}

You can change the parameters and decoder code when creating a converter or editing. If the converter has already been created, click the pencil icon to edit it. Copy the sample converter configuration (or use your own configuration) and paste it into the decoder function. Then save the changes by clicking the checkmark icon.


### **Create Integration**

**Configure kafka integration**

After creating the Uplink converter, it is possible to create an integration.

So in ThingsBoard instance open the Integration menu, select Add integration or Edit action, Details tab.

Set Name, Choose type and select your Uplink data converter from dropdown menu. And fill other required fields:  Topics, Bootstrap server, properties.

{% include images-gallery.html imageCollection="Kafka_confluent_create_integration" %}

In Confluent select the created environment, then open Cluster, Cluster settings.

{% include images-gallery.html imageCollection="Kafka_cloud_bootstrap_0" %}

After, find Bootstrap server URL, it looks like **YOUR-CODE.westeurope.azure.confluent.cloud:9092**

You should copy it to integration:

{% include images-gallery.html imageCollection="Kafka_cloud_bootstrap_1" %}

Also, need will be to add several properties, namely:

<table>
    <tr>
        <th>Key</th>
        <th>Value</th>
    </tr>
    <tr>
        <td>ssl.endpoint.identification.algorithm</td>
        <td>https</td>
    </tr>
    <tr>
        <td>sasl.mechanism</td>
        <td>PLAIN</td>
    </tr>
    <tr>
        <td>sasl.jaas.config</td>
        <td>org.apache.kafka.common.security.plain.PlainLoginModule required username="CLUSTER_API_KEY" password="CLUSTER_API_SECRET";</td>
    </tr>
    <tr>
        <td>security.protocol</td>
        <td>SASL_SSL</td>
    </tr>
</table>

 - **`CLUSTER_API_KEY`** - your access key from Cluster settings.
 - **`CLUSTER_API_SECRET`** - your access secret from Cluster settings.
<br/>
 
To generate the required API key and secret for it, in the cluster you must go to the Data Integration menu, select the API Keys submenu, pick Create key and Select the Scope for the API Key. Here you will see the key and secret to it, which should be used in the integration properties.

{% include images-gallery.html imageCollection="kafka_integration_properties_1" %}

It remains to create a topic on Confluent. To do this, select the "Topics" menu, select "Create Topics", set the name to **my-topic** (It is important that the topics coincide with the specified in the integration. At the next stage, if necessary, you can change the Storage and Message size parameters, and then confirm the creation by the Create with defaults button.

{% include images-gallery.html imageCollection="kafka_confluent_create_topic" %}

With these settings, the integration will request updates from the Kafka broker every 5 seconds.

**Send test Uplink message from Confluent Cloud**

You can simulate a message from the Confluent cloud to ThingsBoard, for this use the available Confluent functionality.
Navigate to the topics in the cluster, the Messages tab, and select the **Produce a new message to this topic**.

{% include images-gallery.html imageCollection="kafka_confluent_send_payload_to_tb" %}

Result matches all keys, timestamps and values:

{% include images-gallery.html imageCollection="Kafka_integration_test_send_msg_result" %}